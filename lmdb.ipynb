{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "from data import BedsoreDataset\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = Path('data/VOCdevkit/VOC2007/JPEGImages/').rglob('*.jpg')\n",
    "image_list = list(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1528/1528 [00:04<00:00, 344.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# creat lmdb dataset\n",
    "\n",
    "env = lmdb.open('JPEGImages_db',map_size=int(1e10))\n",
    "for i in tqdm(image_list):\n",
    "    name = i.name[:-4]\n",
    "    with open(i, 'rb') as f:\n",
    "        image_bin = f.read()\n",
    "    with env.begin(write=True) as txn:\n",
    "        txn.put(name.encode(), image_bin)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1528/1528 [02:03<00:00, 12.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# save arr to  lmdb dataset\n",
    "\n",
    "env = lmdb.open('Byte_db',map_size=int(1e11))\n",
    "for i in tqdm(image_list):\n",
    "    name = i.name[:-4]\n",
    "    arr = cv2.imread(str(i))\n",
    "    arr = arr.tobytes()\n",
    "    with env.begin(write=True) as txn:\n",
    "        txn.put(name.encode(), arr)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1528/1528 [01:22<00:00, 18.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 9.31 s, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in tqdm(image_list):\n",
    "    image = Image.open(i)\n",
    "    arr = np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1528it [01:04, 23.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 3.76 s, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## read lmdb\n",
    "%%time\n",
    "env = lmdb.open('JPEGImages_db')\n",
    "with env.begin(write=False, buffers=True) as txn:\n",
    "    for key, value in tqdm(txn.cursor()):\n",
    "        name = key\n",
    "        image_bin = value\n",
    "        image_buf = np.frombuffer(image_bin, dtype=np.uint8)\n",
    "        img = cv2.imdecode(image_buf, cv2.IMREAD_COLOR)\n",
    "#         img = torch.from_numpy(img)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## read lmdb\n",
    "\n",
    "env = lmdb.open('Byte_db')\n",
    "with env.begin(write=False,buffers=True) as txn:\n",
    "    for key, value in tqdm(txn.cursor()):\n",
    "        name = key\n",
    "        image_bin = value\n",
    "        image_buf = np.frombuffer(image_bin, dtype=np.uint8)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load arr form lmdb\n",
    "env = lmdb.open('JPEGImages')\n",
    "with env.begin(write=False) as txn:\n",
    "    image_bin = txn.get('0'.encode())\n",
    "    image_buf = np.frombuffer(image_bin, dtype=np.uint8)\n",
    "    img = cv2.imdecode(image_buf, cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zrway/anaconda3/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
      "/home/zrway/anaconda3/lib/python3.7/site-packages/skimage/morphology/_skeletonize.py:241: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  0, 1, 1, 0, 0, 1, 0, 0, 0], dtype=np.bool)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = torchvision.datasets.VOCDetection('data',year='2007',image_set='trainval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make byte_db\n",
    "env = lmdb.open('Byte_db',map_size=int(3e10))\n",
    "with env.begin(write=True) as txn:\n",
    "    for i,d in enumerate(tqdm(ds)):\n",
    "        fname = d[1]['annotation']['filename'][:-4]\n",
    "        mask_class_path = f'data/VOCdevkit/VOC2007/SegmentationClass/{fname}.png'\n",
    "        mask_object_path = f'data/VOCdevkit/VOC2007/SegmentationObject/{fname}.png'\n",
    "        if os.path.exists(mask_class_path):\n",
    "            mask = Image.open(mask_object_path).convert('L')\n",
    "            mask_class = Image.open(mask_class_path).convert('L')\n",
    "            mask_byte = np.array(mask).tobytes()\n",
    "            mask_class_byte = np.array(mask_class).tobytes()\n",
    "            arr = np.array(d[0])\n",
    "            byte = arr.tobytes()\n",
    "            anno = str(d[1])\n",
    "            txn.put(f'data_{i}'.encode(), byte)\n",
    "            txn.put(f'anno_{i}'.encode(), anno.encode())\n",
    "            txn.put(f'mask_{i}'.encode(), mask_byte)\n",
    "            txn.put(f'mask_class_{i}'.encode(), mask_class_byte)\n",
    "        else:\n",
    "            print(fname,'not exist')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval() arg 1 must be a string, bytes or code object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-07d798f29d3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data_{id}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0manno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'anno_{id}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0manno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'height'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: eval() arg 1 must be a string, bytes or code object"
     ]
    }
   ],
   "source": [
    "id = 3\n",
    "env = lmdb.open('data/arr_lmdb')\n",
    "with env.begin(write=False) as txn:\n",
    "    arr = txn.get(f'data_{id}'.encode())\n",
    "    anno = txn.get(f'anno_{id}'.encode())\n",
    "    anno = eval(anno)\n",
    "    w = anno['annotation']['size']['width']\n",
    "    h = anno['annotation']['size']['height']\n",
    "    arr = np.frombuffer(arr, dtype=np.uint8)\n",
    "    arr = arr.reshape(int(h),int(w),3)\n",
    "    \n",
    "    mask = txn.get(f'mask_{id}'.encode())\n",
    "    if mask is not None:\n",
    "        mask = np.frombuffer(mask, dtype=np.uint8)\n",
    "        mask = mask.reshape(int(h),int(w))\n",
    "\n",
    "        mask_class = txn.get(f'mask_class_{id}'.encode())\n",
    "        mask_class = np.frombuffer(mask_class, dtype=np.uint8)\n",
    "        mask_class = mask_class.reshape(int(h),int(w))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 137, 173], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mask_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAGoCAAAAABTNNxjAAAD9UlEQVR4nO3dUXLaMBAGYLXTG3IYzuDD5I59SJp0BhsseyWtzPc9JUxiGP2stEiElAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHAhv9pefimllHvb+7iyHvGUIqKD2saz/Pe1gA7oF0+RUL2m8SwPtwiozu+G135MZ+0mnmgZz5pFQDUaxrMRhHwqtItnMwb57NesNXgagg5hpz+Nrpu4RD6+v7oNfBT7NKqeV+mMK5+Px5sSpzQonlH5rIRTSsmbUJPJLevMthVOKR9JA2pRPbvS6V8+2+F8SZhQfPXMVzo/P5EuoPDq2Z1O5/J5nU4pJV0FBVdPUOks0dntDCddBcVWT006Gwm0OcDbHU/JVUCh8VTVzurwL69/5IiadFLlExlP5cy2MvitDojq4kmUT+CW6Pl1J8sBUWWaDcXF02ggIy5bPdxp8gmLJ2AY1y8x5AAvSz69T0ufSHVAlCSfgfE4134tqnM7ONTfjVnT07tDpZCifWt1HLfTUu4l6TbdR4Z8BsfTJZqD60iGfBK1BjyaIZ5RJ98JurcZ4hlmfD5B8aRc3C9gcGO9S9Xk9u+B/PdLJ4pgdHcwvHMLtDx885nRbfwkddQEa8+JzmD6jYkJ4jllKeXMFDW67q4Tz0alnMxnsPnXnlfz13LqddPgrYP81RPyonTW8skfzws7Fv+J+4PZ49k79Lej9TO2OYiKJ/UfVH22BzNOcJO3BlXz1m10LdSbe3KrXlVmK6GwtyG2W3+fzJsn3m+/u46GBhpWPakXnxW3KQpp8rXnlAnWornXntOyl1BcPLPNbl9yT3KB1TNpPqlL6M0nty9PArrOlui05VNGp7AptHra5NNnR3OjgAbHZnL7lrGAYuPpPb3F3t9KPqMjC66e+8zrT8IWLnzX4J7x8Gv3k+aW7PPE4jd1UuZT4fYZUIJsSqMPDAsO6NlzP+nHK0VpsiX6MxwRQc07uOc1bqxTDG2KB3FM69c9rVu5HZefOJ0OL0vn7rUHa/wPYj6dW4FexPv84pM/N7qclo7qtSfPpnSqnlP182qMty49fzblClui6zFcZMXrFM/xwTr0mxcJp1/1NByvh0tfJpxrvJHqft1/IdipNShHu4P9w33uz6yS6hfPsXwuOOQ1OnZuR0b6zdPpWT319fPu4XSOJ+CDrt9M33gq8hFOKd3jufrpZrTe8ZRy8T1mAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGOEvZ+p/zCD9m5gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=414x424 at 0x7F298468AB10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(mask_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import albumentations as album\n",
    "from albumentations.pytorch import ToTensorV2, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedsoreDataset_lmdb(object):\n",
    "    def __init__(self, root='data', transforms=None, image_set='train'):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.data = torchvision.datasets.VOCDetection(root,\n",
    "                                                      year='2007',\n",
    "                                                      image_set=image_set)\n",
    "        self.label_dict = {\n",
    "            '1期': 1,\n",
    "            '2期': 2,\n",
    "            '3期': 3,\n",
    "            '4期': 4,\n",
    "            '不可分期': 5,\n",
    "            '深部组织损伤': 6\n",
    "        }\n",
    "        env = lmdb.open('data/arr_lmdb')\n",
    "        self.txn = env.begin(write=False)\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         image = self.data[idx][0]#PIL\n",
    "        \n",
    "            anno = self.txn.get(f'anno_{idx}'.encode()) # sub self.data[idx][1]\n",
    "            anno = eval(anno)\n",
    "            w = anno['annotation']['size']['width']\n",
    "            h = anno['annotation']['size']['height']\n",
    "\n",
    "            image = self.txn.get(f'data_{idx}'.encode()) \n",
    "            image = np.frombuffer(image, dtype=np.uint8)#arr\n",
    "            image = image.reshape(int(h),int(w),3)\n",
    "\n",
    "            boxes, labels = [], []\n",
    "            image_id = torch.tensor([idx])\n",
    "\n",
    "            fname = anno['annotation']['filename'][:-4]\n",
    "\n",
    "            mask = self.txn.get(f'mask_{idx}'.encode())\n",
    "            if mask is not None:\n",
    "                mask = np.frombuffer(mask, dtype=np.uint8)\n",
    "                mask = mask.reshape(int(h),int(w))\n",
    "                obj_ids = np.unique(mask)[1:]\n",
    "                masks = mask == obj_ids[:, None, None]\n",
    "                mask_class = self.txn.get(f'mask_class_{idx}'.encode())\n",
    "                mask_class = np.frombuffer(mask_class, dtype=np.uint8)\n",
    "                mask_class = mask_class.reshape(int(h),int(w))\n",
    "                mask_class = masks * mask_class\n",
    "                mask_label = mask_class.max(1).max(1).tolist()\n",
    "                ccc = {\n",
    "                    137: 7,\n",
    "                    173: 8,\n",
    "                    98: 9\n",
    "                }  # 137-7:Necrotic,173-8:slough,98-9:Granulation\n",
    "                mask_label = [ccc[i] if i in ccc else i for i in mask_label]\n",
    "                num_objs = len(obj_ids)\n",
    "                mask_boxes = []\n",
    "                for i in range(num_objs):\n",
    "                    pos = np.where(masks[i])\n",
    "                    xmin = float(np.min(pos[1]))\n",
    "                    xmax = float(np.max(pos[1]))\n",
    "                    ymin = float(np.min(pos[0]))\n",
    "                    ymax = float(np.max(pos[0]))\n",
    "                    mask_boxes.append([xmin, ymin, xmax, ymax])\n",
    "            else:\n",
    "                masks = None\n",
    "                mask_boxes = []\n",
    "                mask_label = []\n",
    "\n",
    "            if isinstance(anno['annotation']['object'], list):\n",
    "                for i in anno['annotation']['object']:\n",
    "                    bbox = list(i['bndbox'].values())\n",
    "                    bbox = list(map(float, bbox))\n",
    "                    boxes.append(bbox)\n",
    "                    labels.append(self.label_dict[i['name']])\n",
    "            else:\n",
    "                bbox = list(\n",
    "                    anno['annotation']['object']['bndbox'].values())\n",
    "                bbox = list(map(float, bbox))\n",
    "                boxes.append(bbox)\n",
    "                labels.append(self.label_dict[anno['annotation']\n",
    "                                              ['object']['name']])\n",
    "\n",
    "            pre_masks = torch.zeros(len(labels), image.shape[0], image.shape[1])\n",
    "            labels = labels + mask_label\n",
    "            target = {}\n",
    "            boxes = boxes + mask_boxes\n",
    "            target['boxes'] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            target['labels'] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            target['image_id'] = torch.as_tensor(image_id, dtype=torch.int64)\n",
    "            target['fname'] = fname\n",
    "\n",
    "            if masks is not None:\n",
    "                masks = torch.from_numpy(masks)\n",
    "                masks = torch.cat((pre_masks, masks), dim=0)\n",
    "                target['masks'] = masks\n",
    "            else:\n",
    "                target['masks'] = pre_masks\n",
    "\n",
    "            if self.transforms is not None:\n",
    "                transformed = self.transforms(image=image,\n",
    "                                              bboxes=target['boxes'],\n",
    "                                              mask=target['masks'].permute(\n",
    "                                                  1, 2, 0).numpy(),\n",
    "                                              category_ids=target['labels'])\n",
    "                image = transformed['image']\n",
    "                target['boxes'] = torch.as_tensor(transformed['bboxes'],\n",
    "                                                  dtype=torch.float32)\n",
    "                target['masks'] = transformed['mask'][0].permute(2, 0, 1)\n",
    "\n",
    "            return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/pytorch/transforms.py:58: DeprecationWarning: ToTensor is deprecated and will be replaced by ToTensorV2 in albumentations 0.5.0\n",
      "  \"ToTensor is deprecated and will be replaced by ToTensorV2 \" \"in albumentations 0.5.0\", DeprecationWarning\n"
     ]
    }
   ],
   "source": [
    "trans_prob=0.5\n",
    "tfmc_train = album.Compose(\n",
    "            [\n",
    "                #  album.RandomScale(p=trans_prob, scale_limit=0.5),\n",
    "                #  album.RandomShadow(p=trans_prob),\n",
    "                album.RandomSizedBBoxSafeCrop(\n",
    "                    800, 800, p=0.5, erosion_rate=0.2),\n",
    "                album.HorizontalFlip(p=trans_prob),\n",
    "                album.VerticalFlip(p=trans_prob),\n",
    "                album.ShiftScaleRotate(p=trans_prob, rotate_limit=90),\n",
    "                album.RandomBrightnessContrast(p=trans_prob),\n",
    "                #  album.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.3),\n",
    "                ToTensor()\n",
    "            ],\n",
    "            bbox_params=album.BboxParams(format='pascal_voc',\n",
    "                                         label_fields=['category_ids']))\n",
    "ds = BedsoreDataset_lmdb(transforms=tfmc_train, image_set='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import utils\n",
    "dl = DataLoader(ds,batch_size=8,num_workers=8, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 53/168 [00:19<00:41,  2.76it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 5.\nOriginal Traceback (most recent call last):\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-74-bc4e10ec2486>\", line 104, in __getitem__\n    category_ids=target['labels'])\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/core/composition.py\", line 180, in __call__\n    p.preprocess(data)\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/core/utils.py\", line 62, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=\"to\")\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/core/utils.py\", line 70, in check_and_convert\n    return self.convert_to_albumentations(data, rows, cols)\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\", line 51, in convert_to_albumentations\n    return convert_bboxes_to_albumentations(data, self.params.format, rows, cols, check_validity=True)\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\", line 303, in convert_bboxes_to_albumentations\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\", line 303, in <listcomp>\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\", line 251, in convert_bbox_to_albumentations\n    check_bbox(bbox)\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\", line 330, in check_bbox\n    \"to be in the range [0.0, 1.0], got {value}.\".format(bbox=bbox, name=name, value=value)\nValueError: Expected y_max for bbox (tensor(0.2071), tensor(0.3578), tensor(0.6276), tensor(1.1563), tensor(4)) to be in the range [0.0, 1.0], got 1.1563303470611572.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 5.\nOriginal Traceback (most recent call last):\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-74-bc4e10ec2486>\", line 104, in __getitem__\n    category_ids=target['labels'])\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/core/composition.py\", line 180, in __call__\n    p.preprocess(data)\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/core/utils.py\", line 62, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=\"to\")\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/core/utils.py\", line 70, in check_and_convert\n    return self.convert_to_albumentations(data, rows, cols)\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\", line 51, in convert_to_albumentations\n    return convert_bboxes_to_albumentations(data, self.params.format, rows, cols, check_validity=True)\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\", line 303, in convert_bboxes_to_albumentations\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\", line 303, in <listcomp>\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\", line 251, in convert_bbox_to_albumentations\n    check_bbox(bbox)\n  File \"/home/zrway/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\", line 330, in check_bbox\n    \"to be in the range [0.0, 1.0], got {value}.\".format(bbox=bbox, name=name, value=value)\nValueError: Expected y_max for bbox (tensor(0.2071), tensor(0.3578), tensor(0.6276), tensor(1.1563), tensor(4)) to be in the range [0.0, 1.0], got 1.1563303470611572.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in tqdm(dl):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import BedsoreDataset,BedsoreDataModule\n",
    "ds = BedsoreDataset(transforms=tfmc_train,image_set='train')\n",
    "dl = DataLoader(ds,batch_size=8,num_workers=8, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [01:31<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.6 s, sys: 5.66 s, total: 8.26 s\n",
      "Wall time: 1min 31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in tqdm(dl):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondaf63bac1a07634c71aa70aecffb3be0e1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
